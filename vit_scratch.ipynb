{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e427ca1",
   "metadata": {},
   "source": [
    "## VISION TRANSFORMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea27033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version 2.3.0 with GPU...\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import einops\n",
    "import math\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# Check for GPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using PyTorch version {torch.__version__} with GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "82747709",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "patch_size = 2\n",
    "embed_dim = 512\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size, channels, embed_dim):\n",
    "        \"\"\" Initialize Patch Embedding class \"\"\"\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear = nn.Linear(self.patch_size**2 * self.channels, self.embed_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # data should be channel first format\n",
    "        patches = rearrange(data, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "                            p1=self.patch_size, p2=self.patch_size)\n",
    "        \n",
    "        out = self.linear(patches)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, N, embed_dim):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, N+1, embed_dim))\n",
    "    def forward(self, x):\n",
    "        return x + self.pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "86872f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 10])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(1, 601).reshape(2, 3, 10, 10)\n",
    "X = torch.Tensor(X)\n",
    "N = X.shape[2]*X.shape[3]//(patch_size**2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9f4543c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 25, 512])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embed = PatchEmbedding(patch_size=patch_size, embed_dim=512, channels=3)\n",
    "pe = PositionalEmbedding(N=N, embed_dim=512)\n",
    "out = patch_embed(X)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a25604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "\n",
    "        attn_out, attn_weights = self.attn(q, k, v)\n",
    "        return attn_out, attn_weights\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines an Encoder block as: Multihead + (Add & Norm) + FFN + (Add & Norm)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, expansion_factor, num_heads):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.attention = Attention(num_heads=num_heads, embed_dim=d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, expansion_factor*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(expansion_factor*d_model, d_model)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # output from attention\n",
    "        attn_out, _ = self.attention(X)\n",
    "\n",
    "        # do add and normalize\n",
    "        assert attn_out.shape == X.shape, \"Tensor size do not match!\"\n",
    "        res_connection = attn_out + X\n",
    "        norm1_out = self.dropout1(self.norm1(res_connection))\n",
    "\n",
    "        # do Feedforward\n",
    "        feedfwd_out = self.feed_forward(norm1_out)\n",
    "\n",
    "        # do add and normalize\n",
    "        feed_fwd_residual_out = feedfwd_out + norm1_out\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out))\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "## Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, num_layers=6, expansion_factor=4, num_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.embedding_layer = PatchEmbedding(patch_size=patch_size, channels=3, embed_dim=d_model)\n",
    "        self.positional_encoder = PositionalEmbedding(embed_dim=512, N=N)\n",
    "        self.layers = nn.ModuleList(\n",
    "            EncoderBlock(d_model=d_model, expansion_factor=expansion_factor, num_heads=num_heads) for i in range(num_layers)\n",
    "            )\n",
    "\n",
    "    def forward(self, X):\n",
    "        E = self.embedding_layer(X)\n",
    "        out = self.positional_encoder(E)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ea8614c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attention(embed_dim=512, n_heads=8, dropout=0.1)\n",
    "attn_out, attn_weights = attn(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbd594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
